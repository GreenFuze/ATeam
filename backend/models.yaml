models:
  chatgpt-4o-latest:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "chatgpt\u20114o\u2011latest model from openai"
    id: chatgpt-4o-latest
    model_settings: {}
    name: chatgpt-4o-latest
    provider: openai
  gpt-3.5-turbo:
    context_window_size: 16385
    default_inference:
      max_tokens: 4096
      stream: true
    description: "gpt\u20113.5-turbo model from openai"
    id: gpt-3.5-turbo
    model_settings: {}
    name: gpt-3.5-turbo
    provider: openai
  gpt-3.5-turbo-16k:
    context_window_size: 16385
    default_inference:
      max_tokens: 4096
      stream: true
    description: "gpt\u20113.5-turbo-16k model from openai"
    id: gpt-3.5-turbo-16k
    model_settings: {}
    name: gpt-3.5-turbo-16k
    provider: openai
  gpt-3.5-turbo-instruct:
    context_window_size: 16385
    default_inference:
      max_tokens: 4096
      stream: true
    description: "gpt\u20113.5\u2011turbo\u2011instruct model from openai"
    id: gpt-3.5-turbo-instruct
    model_settings: {}
    name: gpt-3.5-turbo-instruct
    provider: openai
  gpt-4:
    context_window_size: 8192
    default_inference:
      max_tokens: 4096
      stream: true
    description: "gpt\u20114 model from openai"
    id: gpt-4
    model_settings: {}
    name: gpt-4
    provider: openai
  gpt-4-0125-preview:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: gpt-4-0125-preview model from openai
    id: gpt-4-0125-preview
    model_settings: {}
    name: gpt-4-0125-preview
    provider: openai
  gpt-4-1106-preview:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: gpt-4-1106-preview model from openai
    id: gpt-4-1106-preview
    model_settings: {}
    name: gpt-4-1106-preview
    provider: openai
  gpt-4-32k:
    context_window_size: 32768
    default_inference:
      max_tokens: 4096
      stream: true
    description: "gpt\u20114-32k model from openai"
    id: gpt-4-32k
    model_settings: {}
    name: gpt-4-32k
    provider: openai
  gpt-4-turbo:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "gpt\u20114-turbo model from openai"
    id: gpt-4-turbo
    model_settings: {}
    name: gpt-4-turbo
    provider: openai
  gpt-4-turbo-2024-04-09:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: gpt-4-turbo-2024-04-09 model from openai
    id: gpt-4-turbo-2024-04-09
    model_settings: {}
    name: gpt-4-turbo-2024-04-09
    provider: openai
  gpt-4.1:
    context_window_size: 1047576
    default_inference:
      max_tokens: 32768
      stream: true
    description: "gpt\u20114.1 model from openai"
    id: gpt-4.1
    model_settings: {}
    name: gpt-4.1
    provider: openai
  gpt-4.1-mini:
    context_window_size: 1047576
    default_inference:
      max_tokens: 32768
      stream: true
    description: "gpt\u20114.1\u2011mini model from openai"
    id: gpt-4.1-mini
    model_settings: {}
    name: gpt-4.1-mini
    provider: openai
  gpt-4.1-nano:
    context_window_size: 1047576
    default_inference:
      max_tokens: 32768
      stream: true
    description: "gpt\u20114.1\u2011nano model from openai"
    id: gpt-4.1-nano
    model_settings: {}
    name: gpt-4.1-nano
    provider: openai
  gpt-4.5-preview:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: gpt-4.5-preview model from openai
    id: gpt-4.5-preview
    model_settings: {}
    name: gpt-4.5-preview
    provider: openai
  gpt-4.5-preview-2025-02-27:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: gpt-4.5-preview-2025-02-27 model from openai
    id: gpt-4.5-preview-2025-02-27
    model_settings: {}
    name: gpt-4.5-preview-2025-02-27
    provider: openai
  gpt-4o:
    context_window_size: 128000
    default_inference:
      json_object: false
      max_tokens: 16384
      stream: true
    description: "gpt\u20114o model from openai"
    id: gpt-4o
    model_settings: {}
    name: gpt-4o
    provider: openai
  gpt-4o-audio-preview:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "gpt\u20114o\u2011audio\u2011preview model from openai"
    id: gpt-4o-audio-preview
    model_settings: {}
    name: gpt-4o-audio-preview
    provider: openai
  gpt-4o-audio-preview-2024-10-01:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "gpt\u20114o\u2011audio\u2011preview\u20112024\u201110\u201101 model\
      \ from openai"
    id: gpt-4o-audio-preview-2024-10-01
    model_settings: {}
    name: gpt-4o-audio-preview-2024-10-01
    provider: openai
  gpt-4o-audio-preview-2024-12-17:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "gpt\u20114o\u2011audio\u2011preview\u20112024\u201112\u201117 model\
      \ from openai"
    id: gpt-4o-audio-preview-2024-12-17
    model_settings: {}
    name: gpt-4o-audio-preview-2024-12-17
    provider: openai
  gpt-4o-mini:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
      temperature: 0.3
    description: "gpt\u20114o-mini model from openai"
    id: gpt-4o-mini
    model_settings: {}
    name: gpt-4o-mini
    provider: openai
  gpt-4o-mini-audio-preview:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "gpt\u20114o\u2011mini\u2011audio\u2011preview model from openai"
    id: gpt-4o-mini-audio-preview
    model_settings: {}
    name: gpt-4o-mini-audio-preview
    provider: openai
  gpt-4o-mini-audio-preview-2024-12-17:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "gpt\u20114o\u2011mini\u2011audio\u2011preview\u20112024\u201112\u2011\
      17 model from openai"
    id: gpt-4o-mini-audio-preview-2024-12-17
    model_settings: {}
    name: gpt-4o-mini-audio-preview-2024-12-17
    provider: openai
  gpt-oss:20b:
    context_window_size: 131072
    default_inference:
      stream: true
      temperature: 0.5
    description: gpt-oss:20b model from ollama (chat + embedding)
    id: gpt-oss:20b
    model_settings: {}
    name: gpt-oss:20b
    provider: ollama
  o1:
    context_window_size: 200000
    default_inference:
      max_tokens: 100000
      stream: false
    description: o1 model from openai
    id: o1
    model_settings: {}
    name: o1
    provider: openai
  o1-2024-12-17:
    context_window_size: 200000
    default_inference:
      max_tokens: 100000
      stream: false
    description: "o1\u20112024\u201112\u201117 model from openai"
    id: o1-2024-12-17
    model_settings: {}
    name: o1-2024-12-17
    provider: openai
  o1-mini:
    context_window_size: 200000
    default_inference:
      max_tokens: 100000
      stream: true
    description: "o1\u2011mini model from openai"
    id: o1-mini
    model_settings: {}
    name: o1-mini
    provider: openai
  o1-preview:
    context_window_size: 200000
    default_inference:
      max_tokens: 100000
      stream: true
    description: "o1\u2011preview model from openai"
    id: o1-preview
    model_settings: {}
    name: o1-preview
    provider: openai
  o3:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: o3 model from openai
    id: o3
    model_settings: {}
    name: o3
    provider: openai
  o3-mini:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: "o3\u2011mini model from openai"
    id: o3-mini
    model_settings: {}
    name: o3-mini
    provider: openai
  o4-mini:
    context_window_size: 128000
    default_inference:
      max_tokens: 16384
      stream: true
    description: o4-mini model from openai
    id: o4-mini
    model_settings: {}
    name: o4-mini
    provider: openai
